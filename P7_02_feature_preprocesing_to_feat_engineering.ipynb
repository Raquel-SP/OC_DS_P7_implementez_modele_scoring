{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d2bad4-9242-42b6-a70e-6db78140bfda",
   "metadata": {
    "id": "376c512c-b7fd-49c0-b147-8c42d1be918c"
   },
   "source": [
    "\n",
    "\n",
    "# <font size=\"+3\"><span style='color:#2994ff'> **P7 - Implémentez un modèle de scoring** </span></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f999c-2feb-4950-b1d8-705b4af1f1b3",
   "metadata": {
    "id": "967cb3e2-cb54-4c6c-b5b7-411bb3559d31"
   },
   "source": [
    "<a id='LOADING_LIBRARIES'></a>\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "<font size=\"+1\"> **LOADING THE LIBRARIES** </font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc044096-53e2-496b-932a-bd32969f564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of used libraries :\n",
      "Python                : 3.9.13 (main, Aug 25 2022, 23:26:10) \n",
      "[GCC 11.2.0]\n",
      "NumPy                 : 1.21.5\n",
      "Pandas                : 1.5.3\n",
      "Dataframe tools      : 0.0.0\n",
      "Launched on : 2023-07-14T09:10:04.587357\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------\n",
    "# Packages Update\n",
    "# ----------------\n",
    "# !pip install --upgrade pip\n",
    "# !pip install xgboost\n",
    "# !pip install pycodestyle\n",
    "# !pip install flake8 pycodestyle_magic\n",
    "\n",
    "# General libraries\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import tools_dataframe\n",
    "import tools_preprocessing\n",
    "import tools_feat_engineering\n",
    "\n",
    "\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Validation code PEP8\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on\n",
    "\n",
    "# Warnings\n",
    "# import warnings\n",
    "# from warnings import simplefilter\n",
    "# warnings.filterwarnings(action='once')\n",
    "# simplefilter(action='ignore', category=FutureWarning)\n",
    "# simplefilter(action='ignore', category=ValueError)\n",
    "# simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Versions\n",
    "print('Version of used libraries :')\n",
    "\n",
    "print('Python                : ' + sys.version)\n",
    "print('NumPy                 : ' + np.version.full_version)\n",
    "print('Pandas                : ' + pd.__version__)\n",
    "print('Dataframe tools      : ' + tools_dataframe.__version__)\n",
    "now = datetime.now().isoformat()\n",
    "print('Launched on : ' + now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac3cfc-57de-478e-b4a4-8e1df0df0b04",
   "metadata": {
    "id": "9f040c98-873f-4cb4-9917-6cf6775b1399"
   },
   "source": [
    "\n",
    "<a id='USED_PARAMETERS'></a>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "<font size=\"+1\"> **PARAMETERS USED IN THIS NOTEBOOK** </font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73572090-e10f-4dc2-83b2-fc59963648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used in this notebook\n",
    "\n",
    "seed = 84\n",
    "\n",
    "palette4 = [\"#253d85\", \"#618576\", \"#cba87d\", \"#faec4d\"]\n",
    "\n",
    "palette5 = [\"#253d85\", \"#618576\", \"#9595b1\", \"#cba87d\", \"#faec4d\"]\n",
    "\n",
    "palette6 = [\"#253d85\", \"#618576\", \"#9595b1\", \"#cba87d\", \"#dcd2a3\", \"#faec4d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e0a4cf-3e24-41f8-85a4-cefe7c639f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_pd_option = {\n",
    "    'display.max_rows': 400,\n",
    "    'display.max_column': 200,\n",
    "    'display.width': 300,\n",
    "    'display.precision': 4,\n",
    "    'display.max_colwidth': 1000,\n",
    "    'mode.chained_assignment': None\n",
    "}\n",
    "for key, value in dico_pd_option.items():\n",
    "    pd.set_option(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5a0017-c15c-42b2-b8f7-698874cb4abd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e70ac540-a82e-4c8e-b941-e7ed4470f315",
    "outputId": "65181089-44bf-4ba5-9bbf-196ea7d4444f"
   },
   "outputs": [],
   "source": [
    "# Define the folder containing the files with the project data\n",
    "P7_source = \"/home/raquelsp/Documents/Openclassrooms/P7_implementez_modele_scoring/P7_travail/p7_source\"\n",
    "\n",
    "os.chdir(P7_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac096ecf-c949-4ca2-9df9-905aa1eb2c83",
   "metadata": {
    "id": "9f040c98-873f-4cb4-9917-6cf6775b1399"
   },
   "source": [
    "\n",
    "<a id='USED_FUNCTIONS'></a>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "<font size=\"+1\"> **FUNCTIONS USED IN THIS NOTEBOOK** </font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37400648-51e8-4399-b267-9a8f68203fe9",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"+3\"><span style='color:#2994ff'> **P7 - Implémentez un modèle de scoring** </span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a41cb-6792-411f-a61f-5e0fcc461d85",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<center><img src=\"./img/logo_projet.png\" style=\"height: 250px;\"/></center>\n",
    "\n",
    "**Prêt à dépenser** wants to implement a **credit scoring tool to calculate the probability of a customer repaying their loan**, and then **classify the application as either granted or refused credit**. It therefore wants to develop a classification algorithm based on a variety of data sources (behavioural data, data from other financial institutions, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352125b7-b8cf-414f-9e79-78d1d780f7fc",
   "metadata": {},
   "source": [
    "This notebook includes the **pre-processing** of data sets and the **feature engineering**:\n",
    "\n",
    "\n",
    "- application_train/set.csv,\n",
    "\n",
    "- office.csv**,\n",
    "\n",
    "- office_balance.csv**,\n",
    "\n",
    "- credit_card_balance.csv**,\n",
    "\n",
    "- installments_payments.csv**,\n",
    "\n",
    "- POS_CASH_balance.csv**,\n",
    "\n",
    "- previous_application.csv\n",
    "\n",
    "\n",
    "In particular:\n",
    "\n",
    "\n",
    "- Cleaning** :\n",
    "\n",
    "    - memory optimisation** by changing the data type. \n",
    "\n",
    "    - correction of **outliers**\n",
    "    \n",
    "    - correction of duplicated data\n",
    "\n",
    "    - imputation** of missing values\n",
    "\n",
    "- Feature engineering**: in this section, we will start by using the knowledge and insights gained from exploratory data analysis to obtain good sets of variables using feature engineering. Note that most of the variables generated are based on domain knowledge and experimentation. Without good sets of variables, machine learning algorithms cannot produce effective results. We will therefore have to find ingenious ways of doing feature engineering so that the model makes better decisions. \n",
    "\n",
    "- Aggregation**: with main data from Home Crédit Group.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "Sources\n",
    "\n",
    "***\n",
    "\n",
    "The aggregation functions have been taken from the notebook: [Source](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output) by adapting the signature to pass the prefix of the column names and to be able to pass a dictionary of the statistics you want to add."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072282e-bbaa-40b5-aa74-0019fe01fdd9",
   "metadata": {
    "id": "e8622ccb-75b5-4c07-8685-4efcc75630bc",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "## <font color = '#0085dd'>**Table of content**</font>\n",
    "\n",
    "\n",
    "[Librairies loading](#LOADING_LIBRARIES)<br>\n",
    "\n",
    "[Functions used in this notebook](#USED_FUNCTIONS)<br>\n",
    "\n",
    "---\n",
    "\n",
    "[**Datasets**](#datasets)\n",
    " * [Description](#datasets_description)\n",
    "   * [orders_dataset](#orders_dataset)\n",
    "   * [customers_dataset](#customers_dataset)\n",
    "   * [order_items](#order_items)\n",
    "   * [products_dataset](#products_dataset)\n",
    "   * [product_category_name_translation](#product_category_name_translation)   \n",
    "   * [sellers_dataset](#sellers_dataset)\n",
    "   * [order_payments](#order_payments)\n",
    "   * [order_reviews](#order_reviews)\n",
    "   * [geolocation_dataset](#geolocation_dataset)<br> \n",
    "<br>\n",
    " * [Columns preparation](#columns_preparation)\n",
    "   * [Zip codes centroids calculation](#centroid_zipCode)\n",
    "   * [Product category check and update](#cat_products)   \n",
    "   \n",
    "   \n",
    "[**Datasets joining**](#datasets_joining)\n",
    " * [Data description](#data_description)\n",
    " * [Evaluation of missing values](#missing_values)  \n",
    " * [Column filling analysis](#column_fill)   \n",
    " * [Row filling analysis](#row_filling)  \n",
    " * [Features pre-selection](#features_preSelection)\n",
    "\n",
    "[**Features engineering**](#features_engineering)\n",
    " * [Customers spatial distribution](#customers_spatial_distribution)\n",
    " * [RFM features](#RFM_features)  \n",
    " * [Products](#products)   \n",
    " * [Orders](#Orders)  \n",
    " * [Dates](#dates)\n",
    " * [Joining customers information](#join_customers_datasets)\n",
    " * [Features analysis](#features_analysis)  \n",
    "\n",
    "[**Dataset for segmentation**](#segmentation_dataset)\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a742e-89ff-44e0-a17f-2e650bdbc215",
   "metadata": {
    "id": "2f3cf119-1479-4a49-b958-bb22d513aa1f",
    "tags": []
   },
   "source": [
    "<a id='datasets_loading'></a>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# <span style='background:#2994ff'><span style='color:white'>**Train and test datasets** </span></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f0261-0759-44cd-8b8d-9f5993c49e59",
   "metadata": {},
   "source": [
    "| File | Description |\n",
    "| --- | --- |\n",
    "| <p style='text-align: justify;'>**application_train.csv** <br> **application_test.csv**</p> | <ul style='text-align: justify;'><li>The main table, divided into two files for Train (with TARGET) and Test (without TARGET).</li> <li>Static data for all applications.</li><li>A line represents a loan in our data sample.</li></ul> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b3e75-7d5b-4029-bfbb-d4ff0993bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Files upload:\n",
    "# -----------------------------\n",
    "\n",
    "# Train and Test datasets\n",
    "application_train = pd.read_csv('application_train.csv', low_memory=False,\n",
    "                            encoding='utf-8')\n",
    "application_test = pd.read_csv('application_test.csv', low_memory=False,\n",
    "                           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e59c76-1861-487d-bc21-b0730d38ed3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "wbuxtEb26P3y",
    "outputId": "5162fba7-7db6-4eb4-f3fe-8f38f4b2bd32"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Dataset size\n",
    "# --------------\n",
    "nRow, nVar = application_train.shape\n",
    "print(f'The application_train dataset contains {nRow} rows and {nVar} variables.')\n",
    "print()\n",
    "nRow, nVar = application_test.shape\n",
    "print(f'The application_test dataset contains {nRow} rows and {nVar} variables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89d158-7f0d-4371-bfe4-5574efe2c3b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HRTp5goApIcW",
    "outputId": "87a80077-c5b1-43e0-9fa3-7dd2eb67dc3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Variable types\n",
    "# --------------------\n",
    "info_train = tools_dataframe.complet_description(application_train)\n",
    "print(\"train_dataset\")\n",
    "tools_dataframe.visu_dataTypes(info_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03867be-aced-4081-aa1f-565aa6553581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Variable types\n",
    "# --------------------\n",
    "info_test = tools_dataframe.complet_description(application_test)\n",
    "print(\"test_dataset\")\n",
    "tools_dataframe.visu_dataTypes(info_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b098462-3b7c-4029-b09a-9680dbbdd6cd",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_train = application_train.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "cols_num_test = application_test.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2b04d-b2a2-4e71-bcd7-5372824fc574",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cols_cat_train = application_train.select_dtypes(exclude=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "cols_cat_test = application_test.select_dtypes(exclude=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668c38a-d084-4269-9b66-5373aac1359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"application_train\")\n",
    "application_train =\\\n",
    "    tools_preprocessing.reduce_mem_usage(application_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ee588-5384-4e39-88f0-d9a788d86c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"application_test\")\n",
    "application_test =\\\n",
    "    tools_preprocessing.reduce_mem_usage(application_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a8a32-ccdf-47be-8dfc-d983208b8f41",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Manual changes : REGION_RATING_CLIENT contains 1, 2 and 3\n",
    "# ==> object\n",
    "application_train['REGION_RATING_CLIENT'] = \\\n",
    "    application_train['REGION_RATING_CLIENT'].astype('object')\n",
    "# Manual changes : : REGION_RATING_CLIENT_W_CITY contains\n",
    "# ==> 1, 2 ou 3 object\n",
    "application_train['REGION_RATING_CLIENT_W_CITY'] = \\\n",
    "    application_train['REGION_RATING_CLIENT_W_CITY'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be2ab3-c641-4705-ad11-bf8c05ecfa81",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# # Manual changes : REGION_RATING_CLIENT contains 1, 2 and 3\n",
    "# ==> object\n",
    "application_test['REGION_RATING_CLIENT'] = \\\n",
    "    application_test['REGION_RATING_CLIENT'].astype('object')\n",
    "# Manual changes : : REGION_RATING_CLIENT_W_CITY contains\n",
    "# ==> 1, 2 ou 3 object\n",
    "application_test['REGION_RATING_CLIENT_W_CITY'] = \\\n",
    "    application_test['REGION_RATING_CLIENT_W_CITY'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a8f48-639f-499e-9fd6-6e28e3b1bd4b",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# EMERGENCYSTATE_MODE contains Yes/No ==> transform to 1/0\n",
    "# %pycodestyle_off\n",
    "dico_emergency = {'Yes' : 0,\n",
    "                  'No' : 1,\n",
    "                  np.nan : 0}\n",
    "tools_preprocessing.transl_values(application_train,\n",
    "                                  'EMERGENCYSTATE_MODE', dico_emergency)\n",
    "application_train['EMERGENCYSTATE_MODE'] = \\\n",
    "    application_train['EMERGENCYSTATE_MODE'].astype('int8')\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a60657-122f-42d9-b5e8-594ece1b6b8d",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# EMERGENCYSTATE_MODE contains Y/N ==> transformer en 1/0\n",
    "# %pycodestyle_off\n",
    "dico_emergency = {'Yes' : 0,\n",
    "                  'No' : 1,\n",
    "                  np.nan : 0}\n",
    "tools_preprocessing.transl_values(application_test,\n",
    "                                      'EMERGENCYSTATE_MODE', dico_emergency)\n",
    "application_test['EMERGENCYSTATE_MODE'] = \\\n",
    "    application_test['EMERGENCYSTATE_MODE'].astype('int8')\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67da2f-d213-4dfc-ab26-b6956be08c1a",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# FLAG_OWN_CAR contains Y/N ==> transformer en 1/0\n",
    "# %pycodestyle_off\n",
    "dico_owncar = {'Y' : 0,\n",
    "               'N' : 1,\n",
    "               np.nan : 0}\n",
    "tools_preprocessing.transl_values(application_train, 'FLAG_OWN_CAR',\n",
    "                                  dico_owncar)\n",
    "application_train['FLAG_OWN_CAR'] = \\\n",
    "    application_train['FLAG_OWN_CAR'].astype('int8')\n",
    "application_train['FLAG_OWN_CAR'].unique()\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0100a-b111-4033-9138-dc99a940b4c7",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# FLAG_OWN_CAR contains Y/N ==> transformer en 1/0\n",
    "# %pycodestyle_off\n",
    "dico_owncar = {'Y' : 0,\n",
    "               'N' : 1,\n",
    "               np.nan : 0}\n",
    "tools_preprocessing.transl_values(application_test, 'FLAG_OWN_CAR',\n",
    "                                      dico_owncar)\n",
    "application_test['FLAG_OWN_CAR'] = \\\n",
    "    application_test['FLAG_OWN_CAR'].astype('int8')\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819c466-6f50-47d8-b61a-79c8fb7a5a99",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# FLAG_OWN_REALTY contains Y/N ==> transformer en 1/0\n",
    "# %pycodestyle_off\n",
    "dico_ownreal = {'Y' : 0,\n",
    "                'N' : 1,\n",
    "                np.nan : 0}\n",
    "tools_preprocessing.transl_values(application_train, 'FLAG_OWN_REALTY',\n",
    "                                  dico_ownreal)\n",
    "application_train['FLAG_OWN_REALTY'] = \\\n",
    "    application_train['FLAG_OWN_REALTY'].astype('int8')\n",
    "application_train['FLAG_OWN_REALTY'].unique()\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d898a48-5bb1-4b07-b57e-37ea0d118764",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# FLAG_OWN_REALTY contains Y/N ==> transformer en 1/0\n",
    "# %pycodestyle_off\n",
    "dico_ownreal = {'Y' : 0,\n",
    "                'N' : 1,\n",
    "                np.nan : 0}\n",
    "tools_preprocessing.transl_values(application_test, 'FLAG_OWN_REALTY',\n",
    "                                      dico_ownreal)\n",
    "application_test['FLAG_OWN_REALTY'] = \\\n",
    "    application_test['FLAG_OWN_REALTY'].astype('int8')\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a89c6-f150-4bf2-8bb2-e86ece8c8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE UNINFORMATIVE VARIABLES\n",
    "\n",
    "# there are some FLAG_DOCUMENT features having just one category\n",
    "# for almost all data, we will remove those\n",
    "flag_cols_to_drop = ['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\n",
    "                     'FLAG_DOCUMENT_10','FLAG_DOCUMENT_12',\n",
    "                     'FLAG_DOCUMENT_20']\n",
    "application_train = application_train.drop(flag_cols_to_drop, axis = 1)\n",
    "application_test = application_test.drop(flag_cols_to_drop,  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d926c37-f445-4935-89dd-74c2622d0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT TIME VARIABLES\n",
    "\n",
    "#converting age from days to years\n",
    "application_train['DAYS_BIRTH'] = application_train['DAYS_BIRTH'] * -1 / 365\n",
    "application_test['DAYS_BIRTH'] = application_test['DAYS_BIRTH'] * -1 / 365\n",
    "#From the EDA we saw some erroneous values in DAYS_EMPLOYED field\n",
    "application_train['DAYS_EMPLOYED']\\\n",
    "    [application_train['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "application_test['DAYS_EMPLOYED']\\\n",
    "    [application_test['DAYS_EMPLOYED'] == 365243] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76444b36-78cd-41f6-9ecf-9678b0543948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE ERRONEUS VALUES IN OBS COLUMNS\n",
    "\n",
    "application_train['OBS_30_CNT_SOCIAL_CIRCLE']\\\n",
    "    [application_train['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "application_train['OBS_60_CNT_SOCIAL_CIRCLE']\\\n",
    "    [application_train['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "application_test['OBS_30_CNT_SOCIAL_CIRCLE']\\\n",
    "    [application_test['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "application_test['OBS_60_CNT_SOCIAL_CIRCLE']\\\n",
    "    [application_test['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f132e-795c-48a2-87e4-bcee0390ca83",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='application_traintest_preprocessing'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>Application_train/test preprocessing</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab734ae-7229-4bbf-b6cb-4b9e7899d59b",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='application_traintest_outliers'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>Application_train/test outliers</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7168b-4f3e-48b0-97bb-060a5c3ca1f6",
   "metadata": {},
   "source": [
    "Correction of outliers detected during the EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c595f-973c-4bd0-973a-60765faa7dfc",
   "metadata": {},
   "source": [
    "**DAYS_EMPLOYED**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641302a4-8d7d-4de9-b3c9-251c22219620",
   "metadata": {},
   "source": [
    "From the 90th percentile, the value is 365243 days, i.e. 1000 years!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d285b-42a1-4ac8-8820-54775401dbef",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Number of rows with errors\n",
    "print(\"application_train\")\n",
    "nb_err = \\\n",
    "    application_train['DAYS_EMPLOYED'][application_train['DAYS_EMPLOYED']\n",
    "                                       == 365243].count()\n",
    "pourc_err = round((nb_err*100)/application_train.shape[0], 2)\n",
    "print(f'Number of errors DAYS_EMPLOYED : {nb_err} ({pourc_err}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95825b50-986c-4ba0-9e29-3a83a08afb4e",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Correction\n",
    "application_train['DAYS_EMPLOYED'][application_train['DAYS_EMPLOYED']\n",
    "                                   == 365243] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7780767-b75b-40b3-b94a-32416cba6a74",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Number of rows with errors\n",
    "print(\"application_test\")\n",
    "nb_err = \\\n",
    "    application_test['DAYS_EMPLOYED'][application_test['DAYS_EMPLOYED']\n",
    "                                       == 365243].count()\n",
    "pourc_err = round((nb_err*100)/application_test.shape[0], 2)\n",
    "print(f'Number of errors DAYS_EMPLOYED : {nb_err} ({pourc_err}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb4243-6617-4e7c-8e4c-b99a3caf224f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Correction\n",
    "application_test['DAYS_EMPLOYED'][application_test['DAYS_EMPLOYED']\n",
    "                                   == 365243] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d4ee8-266d-4c37-82bb-f8520fdc5bdf",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='application__traintest_duplicated_data'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>Application_train/test duplicated data</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f161d-3a92-4619-927c-d19b2e3c227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicated data in application_train: \",\\\n",
    "      application_train.loc[application_train.duplicated()].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1a640-4fad-457e-883f-25409d94c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicated data in application_test: \",\\\n",
    "      application_test.loc[application_test.duplicated()].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d8a54-e433-4af6-9113-c314072392c8",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='application_traintest_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>Application_train/test missing values</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15485067-9ea2-40e2-b3ab-85aa6385d2e7",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Valeurs manquantes du dataframe\n",
    "df_nan_applitrain = tools_dataframe.get_missing_values(application_train,\n",
    "                                                       True, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f8e1a-92bc-4efb-9b0c-0c61f07a55fb",
   "metadata": {},
   "source": [
    "\n",
    "* From the important variables identified during the exploratory analysis to distinguish non-defaulters from defaulters, the FLOORSMIN_AVG variable has the highest number of missing values (67.85%).\n",
    "* We set the threshold for deleting variables with many missing values at 68%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c23d24-4ac3-4c6d-9612-f05fa2a423cf",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# List of variables with more than 68% of missing values\n",
    "cols_nan_remov_train = \\\n",
    "    df_nan_applitrain[df_nan_applitrain['% of missing values'] > 68] \\\n",
    "    .index.to_list()\n",
    "print(f'Number of variables to remove : {len(cols_nan_remov_train)}')\n",
    "cols_nan_remov_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd00e3-08d9-40ac-93a1-47b715f44374",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove with % of nan over the threshold > 68%\n",
    "application_train.drop(columns=cols_nan_remov_train, inplace=True)\n",
    "\n",
    "# Categorical variables\n",
    "cols_cat_train = application_train.select_dtypes(exclude=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "# Continuous variables\n",
    "cols_num_train = application_train.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "\n",
    "# Size : number of rows/columns\n",
    "nRow, nVar = application_train.shape\n",
    "print(f'The dataset application_train contains {nRow} rows and {nVar} variables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a4256-94de-43e4-81d7-d6530e39beef",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Valeurs manquantes du dataframe\n",
    "df_nan_applitest = tools_dataframe.get_missing_values(application_test,\n",
    "                                                       True, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130c47d0-29f4-4a31-b8e0-36a6fd932dd8",
   "metadata": {},
   "source": [
    "\n",
    "* From the important variables identified during the exploratory analysis to distinguish non-defaulters from defaulters, the FLOORSMIN_AVG variable has the highest number of missing values (66.61%).\n",
    "* We set the threshold for deleting variables with many missing values at 67%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc6648-6e28-456c-a9a5-40a46e3bcea6",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# List of variables with more than 67% of missing values\n",
    "cols_nan_remov_test = \\\n",
    "    df_nan_applitest[df_nan_applitest['% of missing values'] > 67] \\\n",
    "    .index.to_list()\n",
    "print(f'Number of variables to remove : {len(cols_nan_remov_test)}')\n",
    "cols_nan_remov_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86467bdd-b761-4bc6-9ee7-bf63a8791ba3",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove with % of nan over the threshold > 67%\n",
    "application_test.drop(columns=cols_nan_remov_test, inplace=True)\n",
    "\n",
    "# Categorical variables\n",
    "cols_cat_test = application_test.select_dtypes(exclude=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "# Continuous variables\n",
    "cols_num_test = application_test.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "\n",
    "# Size : number of rows/columns\n",
    "nRow, nVar = application_test.shape\n",
    "print(f'The dataset application_test contains {nRow} rows and {nVar} variables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b807f0-c6f6-4a79-9239-9dfe4aa856e3",
   "metadata": {},
   "source": [
    "<a id='application_train_assign_missing_values'></a>\n",
    "\n",
    "---\n",
    "#### <span style='background:#0075bc'><span style='color:white'>Application_train assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34157c08-ac75-4229-8f08-bbb1a52336f7",
   "metadata": {},
   "source": [
    "* During the exploratory analysis, it was observed that values that were not provided could give indications. The applicant may have withheld information to avoid having his application refused.\n",
    "* The EDA also revealed the existence of many outliers.\n",
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6903b08-3117-4b0e-bf19-10b7ac90b2ff",
   "metadata": {},
   "source": [
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce1b80-841f-4524-bd64-44dee0abed7c",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# application_train\n",
    "nb_nan_median = application_train[cols_num_train].isna().sum().sum()\n",
    "print(f'Number of nan in application_train before median assignment : {nb_nan_median}')\n",
    "application_train.fillna(application_train[cols_num_train].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = application_train[cols_num_train].isna().sum().sum()\n",
    "print(f'Number of nan in application_train after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aefc57-b822-46ee-bcf3-2ac6354bc327",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# application_test\n",
    "nb_nan_median = application_test[cols_num_test].isna().sum().sum()\n",
    "print(f'Number of nan in application_test before median assignment : {nb_nan_median}')\n",
    "application_test.fillna(application_test[cols_num_test].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = application_test[cols_num_test].isna().sum().sum()\n",
    "print(f'Number of nan in application_test after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52674e-0283-46b6-8c21-503d20c33602",
   "metadata": {},
   "source": [
    "**Categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea4db9-3645-474b-94bc-c1cbde7078ea",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# application_train\n",
    "nb_nan_cat = application_train[cols_cat_train].isna().sum().sum()\n",
    "print(f'Number of nan in application_train before mode assignment : {nb_nan_cat}')\n",
    "for var in cols_cat_train:\n",
    "    mode = application_train[var].mode()[0]\n",
    "    application_train[var].fillna(mode, inplace=True)\n",
    "nb_nan_cat = application_train[cols_cat_train].isna().sum().sum()\n",
    "print(f'Number of nan in application_train after mode assignment  : {nb_nan_cat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19d882-8be5-4111-a09a-3f447bf75b2f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# application_test\n",
    "nb_nan_cat = application_test[cols_cat_test].isna().sum().sum()\n",
    "print(f'Number of nan in application_test before mode assignment : {nb_nan_cat}')\n",
    "for var in cols_cat_test:\n",
    "    mode = application_test[var].mode()[0]\n",
    "    application_test[var].fillna(mode, inplace=True)\n",
    "nb_nan_cat = application_test[cols_cat_test].isna().sum().sum()\n",
    "print(f'Number of nan in application_test after mode assignment  : {nb_nan_cat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da3904-0681-435a-b1f1-4490bc2801eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "print(f'Number of nan in application_train : {application_train.isna().sum().sum()}')\n",
    "# Saving application_train\n",
    "path_sav_appltrain_wonan = \\\n",
    "    '/home/raquelsp/Documents/Openclassrooms/P7_implementez_modele_scoring/P7_travail/P7_scoring_credit/preprocessing/application_train_wo_nan.pkl'\n",
    "with open(path_sav_appltrain_wonan, 'wb') as f:\n",
    "    pickle.dump(application_train, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b18c6-c5a0-4251-950c-0a0b8aa52bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "print(f'Number of nan in application_test : {application_test.isna().sum().sum()}')\n",
    "# Saving application_train\n",
    "path_sav_appltest_wonan = \\\n",
    "    '/home/raquelsp/Documents/Openclassrooms/P7_implementez_modele_scoring/P7_travail/P7_scoring_credit/preprocessing/application_test_wo_nan.pkl'\n",
    "with open(path_sav_appltest_wonan, 'wb') as f:\n",
    "    pickle.dump(application_test, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d70e81-77fe-41a7-a9b2-a8055fb2984b",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='application_traintest_feat_engi'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>Application_train/test feat engineering</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb744fe7-a316-47b3-8ca8-693a91cf16e0",
   "metadata": {},
   "source": [
    "These tables consists of static data relating to the Borrowers. Each row represents one loan application.\n",
    "\n",
    "<ol><li>First we do feature engineering on the numeric features, and generate features based on Domain Knoweldge, such as INCOME TO ANNUITY ratio, EXT_SOURCE means, etc.</li>\n",
    "    <li>We also try to predict the interest rates by using the data from the previous applications features, and predicting using the data from application_train features. We also create a feature based on the Target values from application_train where we compute the mean of targets of 500 nearest neighbors of each row.</li>\n",
    "    <li>Next we create some features based on the categorical interactions by grouping the data on several categorical combinations and imputing the aggregates for each group as features.</li>\n",
    "    <li>We encode the categorical features by response coding, as we didn't want to increase dimensionality by many-folds using OHE. </li>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5a7f4c4-9045-4e5f-b923-64c8dc65ba9c",
   "metadata": {},
   "source": [
    "# Load de datasets after feature engineering\n",
    "path =  '../P7_scoring_credit/preprocessing/'\n",
    "\n",
    "with open(path + 'application_train_wo_nan.pkl', 'rb') as f:\n",
    "    application_train = pickle.load(f)\n",
    "with open(path + 'application_test_wo_nan.pkl', 'rb') as f:\n",
    "    application_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7208c-9481-4c2f-aa21-f2d46ed62064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "application_train, application_test = tools_feat_engineering.feat_eng_application_train_test(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779822f-d01a-4424-a159-e00898079d5b",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093062ff-5235-4274-a82c-11f9dfd1f1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_temp = '../P7_scoring_credit/preprocessing/temp/'\n",
    "\n",
    "filelist = [ f for f in os.listdir(path_temp) if f.endswith(\".pkl\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(path_temp, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb29431-dc6a-43c4-a916-75d5afa67b88",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='application_traintest_manag_strong_correl'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>Application_train/test Managing strong correlations</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30248e27-4edd-4537-bfa4-48923889c562",
   "metadata": {},
   "source": [
    "In this section:\n",
    " * The correlation between the numerical variables, Pearson's coefficient, is calculated.\n",
    " * The maximum accepted correlation threshold is set at 0.8.\n",
    " * The variables whose correlation exceeds the maximum threshold are suppressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29cdd1-ed44-451b-aca2-8e8e018d3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_application_train_afteng =  '../P7_scoring_credit/preprocessing/application_train_aft_feat_eng.pkl'\n",
    "\n",
    "with open(path_sav_application_train_afteng, 'rb') as f:\n",
    "    application_train_aft_feat_eng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf694c-723f-4dc3-a809-d31ad2a25d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_application_test_afteng =  '../P7_scoring_credit/preprocessing/application_train_aft_feat_eng.pkl'\n",
    "\n",
    "with open(path_sav_application_test_afteng, 'rb') as f:\n",
    "    application_test_aft_feat_eng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301eca5a-6b2d-439c-bae0-bfff6dbd1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"application_train\")\n",
    "application_train =\\\n",
    "    tools_preprocessing.reduce_mem_usage(application_train, verbose=True)\n",
    "\n",
    "print(\"\\napplication_test\")\n",
    "application_test =\\\n",
    "    tools_preprocessing.reduce_mem_usage(application_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbed245-d0a0-410f-a620-10842764ae9a",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX application_train\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix to avoid having to manage\n",
    "# positive and negative correlations separately\n",
    "corr = application_train.corr().abs()\n",
    "\n",
    "# Only the part above the diagonal is retained so that\n",
    "# the correlations are taken into account only once (axial symmetry).\n",
    "corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                           .astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a62efc-df5e-4a01-a4cf-951add1292fc",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables with a Pearson coef > 0.8?\n",
    "cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                    if any(corr_triangle[var] > threshold)]\n",
    "print(f'There are {len(cols_corr_a_supp)} variables in application_train with strong correlation to be removed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d653c4-d683-46dd-889d-6b81f7382a81",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'application_train : {application_train.shape}')\n",
    "application_train.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'application_train : {application_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d96fca-9efe-4b8e-839d-f343f806dc38",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'application_test : {application_test.shape}')\n",
    "application_test.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'application_test : {application_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bc8e0-1e49-4593-bce4-8f509963bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving application_train ready for ML\n",
    "path_sav_appltrain_ML = \\\n",
    "    '../P7_scoring_credit/preprocessing/application_train_ML.pkl'\n",
    "with open(path_sav_appltrain_ML, 'wb') as f:\n",
    "    pickle.dump(application_train, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c054c-d8b2-4831-8da8-4d227f9e5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving application_test ready for ML\n",
    "path_sav_appltest_ML = \\\n",
    "    '../P7_scoring_credit/preprocessing/application_test_ML.pkl'\n",
    "with open(path_sav_appltest_ML, 'wb') as f:\n",
    "    pickle.dump(application_test, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c616b-6e81-4331-94ac-d071057fff4b",
   "metadata": {},
   "source": [
    "**Chargement des dataframes nettoyés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6640f-3187-446a-a3ce-98da19851cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading application_train\n",
    "with open(path_sav_appltrain_ML, 'rb') as df_appli_train_clean:\n",
    "    application_train_ML = pickle.load(df_appli_train_clean)\n",
    "application_train_ML.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a61fb-b807-4df6-8c99-c4a8d014525f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading application_test\n",
    "with open(path_sav_appltest_ML, 'rb') as df_appli_test_clean:\n",
    "    application_test_ML = pickle.load(df_appli_test_clean)\n",
    "application_test_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b29058-ce2c-4fba-bc0f-17600a7bf83c",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfc57a-c14a-4952-bb20-5efedec7a3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['application_train_wo_nan.pkl', 'application_test_wo_nan.pkl', 'application_train_aft_feat_eng.pkl', 'application_test_aft_feat_eng.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47808625-5aa3-4167-9012-b63c68ad1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del application_train, application_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531819c-bece-4ca7-a397-bd754c6e6914",
   "metadata": {
    "id": "2f3cf119-1479-4a49-b958-bb22d513aa1f",
    "tags": []
   },
   "source": [
    "<a id='previous_loans_Home_Credit'></a>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# <span style='background:#2994ff'><span style='color:white'>**Data from previous loans at Home Crédit:** </span></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c91c82-de43-4b01-8a67-90cb23044b58",
   "metadata": {},
   "source": [
    "4 files contain information on previous loans at Home Crédit:\n",
    "* The main file for previous loans :\n",
    "    * **previous_application**\n",
    "* Details of these previous loans :\n",
    "    * **POS_CASH_balance**\n",
    "    * **installments_payments**\n",
    "    * **credit_card_balance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d2f0f-d4b5-404b-87b7-6eeb092da566",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "tags": []
   },
   "source": [
    "<a id='previous_application_preprocessing'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>previous_application</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05259f-843f-4412-a6e0-09158b09d7e8",
   "metadata": {},
   "source": [
    "This table contains the static data related to clients and their previous credits with Home Credit Group.\n",
    "<ol><li>First we start by cleaning the erroneous values. From the EDA we saw some DAYS fields with a value equal to 365243.0, they look erroneous, and so we will be replacing them with NaN values. </li>\n",
    "    <li>We replace the NaN values for categories with an 'XNA' category.</li>\n",
    "    <li>Next, we proceed to feature engineering, where we create some domain based features, such as Credit-Downpayment Ratio, Amount not approved, Credit to Goods ratio, etc.</li>\n",
    "    <li>We also try to predict the interest rate, inspired by one of the writeups of winners. </li>\n",
    "    <li>To be able to merge it with main table, we need to aggregate the rows of previous_application over SK_ID_CURR. We perform domain based aggregations, over all the previous credits for each customer, such as mean, max, min, etc. Here again we aggregate in three ways. First we perform overall aggregation, next we aggregate for first 2 applications and latest 5 applications. The First and Last are decided by the DAYS_FIRST_DUE of applications. In the end, we merge all these aggregations together.</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2d969-ac57-4539-aff5-8b406f1b5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Files upload:\n",
    "# -----------------------------\n",
    "# Data from previous loans at Home Credit\n",
    "previous_application = pd.read_csv('previous_application.csv',\n",
    "                                   low_memory=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e0d8a-b57b-46d3-98ed-1c10c23325d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"previous_application\")\n",
    "previous_application =\\\n",
    "    tools_preprocessing.reduce_mem_usage(previous_application, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f0d71-dd50-481a-906a-7d09062ac9e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HRTp5goApIcW",
    "outputId": "87a80077-c5b1-43e0-9fa3-7dd2eb67dc3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Variable types\n",
    "# --------------------\n",
    "info_previous_application =\\\n",
    "    tools_dataframe.complet_description(previous_application)\n",
    "print(\"previous_application\")\n",
    "tools_dataframe.visu_dataTypes(info_previous_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd35558-4b54-414b-8512-d0d6e0d49ace",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='previous_application_preproc'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>previous_application preprocessing</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e7985-cb75-40ac-9df1-95b061b35d71",
   "metadata": {},
   "source": [
    "<a id='previous_application_outliers'></a>\n",
    "\n",
    "---\n",
    "#### <span style='background:#0075bc'><span style='color:white'>previous_application outliers</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfdea1-8e2e-4e11-99e5-a793d6119e27",
   "metadata": {},
   "source": [
    "In the EDA we found some erroneous values in DAYS columns, so we will replace them with NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8294267-c4e8-49d2-9278-06d7483fda88",
   "metadata": {},
   "source": [
    "**DAYS_FIRST_DRAWING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a21166-2420-484a-b43f-f71fcc9f3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application['DAYS_FIRST_DRAWING'][previous_application['DAYS_FIRST_DRAWING'] == 365243.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eedc8d-2441-42e2-84cc-2518065a2a27",
   "metadata": {},
   "source": [
    "**DAYS_FIRST_DUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2cbfb1-3945-456b-87ac-532e0f9f1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application['DAYS_FIRST_DUE'][previous_application['DAYS_FIRST_DUE'] == 365243.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73db97e-7cc4-44c6-9642-eaa60762097f",
   "metadata": {},
   "source": [
    "**DAYS_LAST_DUE_1ST_VERSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de768835-139f-408c-be72-8867b14ea115",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application['DAYS_LAST_DUE_1ST_VERSION'][previous_application['DAYS_LAST_DUE_1ST_VERSION'] == 365243.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6eae03-ea97-41c9-8861-58eaa5c2b95b",
   "metadata": {},
   "source": [
    "**DAYS_LAST_DUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952b0c3-a349-4a6c-9e44-b66b1c010bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application['DAYS_LAST_DUE'][previous_application['DAYS_LAST_DUE'] == 365243.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff05035-c1c2-4db6-b465-53946d99b1b4",
   "metadata": {},
   "source": [
    "**DAYS_TERMINATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483aea3-f19c-4180-b73d-a693559a8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application['DAYS_TERMINATION'][previous_application['DAYS_TERMINATION'] == 365243.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed7bf2-d18c-4cb5-b337-ba8bcdb318b6",
   "metadata": {},
   "source": [
    "**SELLERPLACE_AREA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ea376-ee71-42d4-b086-72dcd62cb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application['SELLERPLACE_AREA'][previous_application['SELLERPLACE_AREA'] == 4000000] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c79a2-1d6a-437a-b1c5-72febce2e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"previous_application\")\n",
    "previous_application =\\\n",
    "    tools_preprocessing.reduce_mem_usage(previous_application, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646dce19-a58b-4c0e-a637-2482dba8e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau_balance after preprocessing\n",
    "path_sav_prev_app_aftproc = \\\n",
    "    '../P7_scoring_credit/preprocessing/previous_application_aftproc.pkl'\n",
    "with open(path_sav_prev_app_aftproc, 'wb') as f:\n",
    "    pickle.dump(previous_application, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ba7bd-24ad-4cb3-acdc-bb7a96043540",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='previous_application_feat_eng'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>previous_application feature engineering</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b056cd2-5db2-455f-b4ba-849505cf38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application = tools_feat_engineering.feat_eng_previous_application(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c52a2-5405-43b0-a0a1-51f77e7bdbbb",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='previous_application_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>previous_application assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516fd766-f36e-47fb-a26a-0d382bb1f6c3",
   "metadata": {},
   "source": [
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d2a6f-81c5-4908-8747-857d3be9d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_previous_application_afteng =  '../P7_scoring_credit/preprocessing/previous_application_afteng.pkl'\n",
    "\n",
    "with open(path_sav_previous_application_afteng, 'rb') as f:\n",
    "    previous_application_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658ada9-a611-4a11-9190-5023c790e950",
   "metadata": {},
   "source": [
    "\n",
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9be07-0ceb-4e3f-8dd4-1ff120d7bb23",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_prevapp = previous_application_afteng.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88cce7-411d-46c4-9061-f6350a49bde0",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "nb_nan_median = previous_application_afteng[cols_num_prevapp].isna().sum().sum()\n",
    "print(f'Number of nan in previous_application_afteng before median assignment : {nb_nan_median}')\n",
    "previous_application_afteng.fillna(previous_application_afteng[cols_num_prevapp].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = previous_application_afteng[cols_num_prevapp].isna().sum().sum()\n",
    "print(f'Number of nan in previous_application_afteng after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafab5d-93ab-4e24-b9f7-89fd9122e488",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='previous_application_strong_correl'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>previous_application remove strong correlations</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ecfe9-9e48-42cc-a505-3d7d38de3ebd",
   "metadata": {},
   "source": [
    "In this section:\n",
    " * The correlation between the numerical variables, Pearson's coefficient, is calculated.\n",
    " * The maximum accepted correlation threshold is set at 0.8.\n",
    " * The variables whose correlation exceeds the maximum threshold are drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e5ab5-57e7-486e-b7c7-166d459d7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "with open(path_sav_previous_application_afteng, 'rb') as f:\n",
    "    previous_application_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ee5ec-aacb-4488-a1ba-ffeeb4ccac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"previous_application_afteng\")\n",
    "previous_application_afteng =\\\n",
    "    tools_preprocessing.reduce_mem_usage(previous_application_afteng, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7aab8-9c65-4689-9140-d956250175ce",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX bureau_merged\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix to avoid having to manage\n",
    "# positive and negative correlations separately\n",
    "corr = previous_application_afteng.corr().abs()\n",
    "\n",
    "# Only the part above the diagonal is retained so that\n",
    "# the correlations are taken into account only once (axial symmetry).\n",
    "corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                           .astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5141e-92f5-4395-a623-812092acb561",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables with a Pearson coef > 0.8?\n",
    "cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                    if any(corr_triangle[var] > threshold)]\n",
    "print(f'There are {len(cols_corr_a_supp)} variables in previous_application_afteng with strong correlation to be removed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6cfdb-82c8-4128-87b2-3cb401b7bfb6",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'previous_application_afteng : {previous_application_afteng.shape}')\n",
    "previous_application_afteng.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'previous_application_afteng : {previous_application_afteng.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda5fc73-c894-49aa-8082-dc0a7894a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving previous_application_afteng ready for ML\n",
    "path_sav_previous_application_ML = \\\n",
    "    '../P7_scoring_credit/preprocessing/previous_application_ML.pkl'\n",
    "with open(path_sav_previous_application_ML, 'wb') as f:\n",
    "    pickle.dump(previous_application_afteng, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8895af-af97-455e-b6cd-4ae2a7bb4c59",
   "metadata": {},
   "source": [
    "**Loading previous_application data after pre-processing and feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298081e3-dd77-4e51-8d0b-247d15bba40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading previous_application\n",
    "with open(path_sav_previous_application_ML, 'rb') as df_previous_application_ML:\n",
    "    previous_application_ML = pickle.load(df_previous_application_ML)\n",
    "previous_application_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02660327-18e5-427f-951e-b945f455d152",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e3bd2-ca59-4cb8-913e-27ac9514acfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['previous_application_aftproc.pkl', 'previous_application_afteng.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74191577-b730-4c93-9dae-6e2d360b633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del previous_application, previous_application_afteng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543f51e-e70b-473e-9bb8-3a650dc9c51a",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<a id='POS_CASH_balance_preprocessing'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>POS_CASH_balance</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264c1a3-193d-4582-89b5-f1cfc51f73bf",
   "metadata": {},
   "source": [
    "This table contains the Monthly Balance Snapshots of previous Point of Sales and Cash Loans that the applicant had with Home Credit Group. The table contains columns like the status of contract, the number of installments left, etc.\n",
    "\n",
    "<ol><li>Similar to bureau_balance table, this table also has time based features. So we start off by computing the EDAs on CNT_INSTALMENT and CNT_INSTALMENT_FUTURE features. </li>\n",
    "    <li>We create some domain based features next.</li>\n",
    "    <li>We then aggregate the data over SK_ID_PREV. For this aggregation, we do it in 3 ways. Firstly we aggregate the whole data over SK_ID_PREV. We also aggregate the data for last 2 years separately and rest of the years separately. Finally, we also aggregate the data different Contract types, i.e. Active and Completed.</li>\n",
    "    <li>Next, we aggregate the data over SK_ID_CURR, for it to be merged with main table.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeed5c5-1f20-44b2-94cc-e7ddcc1100bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Files upload:\n",
    "# -----------------------------\n",
    "# Data from previous loans at Home Credit\n",
    "POS_CASH_balance = pd.read_csv('POS_CASH_balance.csv',\n",
    "                               low_memory=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc77a43-80ed-4257-b4df-97423677dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"POS_CASH_balance\")\n",
    "POS_CASH_balance =\\\n",
    "    tools_preprocessing.reduce_mem_usage(POS_CASH_balance, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572b54a-24fb-4a86-91fe-41576af866cd",
   "metadata": {},
   "source": [
    "No outliers were found for this file during the exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f0948-d907-4702-9c5c-9f98bd692c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau_balance after preprocessing\n",
    "path_sav_POS_CASH_aftproc = \\\n",
    "    '../P7_scoring_credit/preprocessing/POS_CASH_aftproc.pkl'\n",
    "with open(path_sav_POS_CASH_aftproc, 'wb') as f:\n",
    "    pickle.dump(POS_CASH_balance, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cabee-1d07-45ee-adfd-e1a76b6e03b3",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='POS_CASH_balance_feat_eng'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>POS_CASH_balance feature engineering</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eea4c3-375b-4c85-b593-3add24bfe5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "POS_CASH_balance = tools_feat_engineering.feat_eng_POS_CASH_balance(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8bc6b-3381-4d4f-a664-3770330d8fcc",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='POS_CASH_balance_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>POS_CASH_balance assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c136cf8-1e64-4405-9049-a0f5ddad27a2",
   "metadata": {},
   "source": [
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e742bd-d379-47dd-91d3-a556c5bc4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_POS_CASH_balance_afteng =  '../P7_scoring_credit/preprocessing/POS_CASH_balance_afteng.pkl'\n",
    "\n",
    "with open(path_sav_POS_CASH_balance_afteng, 'rb') as f:\n",
    "    POS_CASH_balance_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6157d-5e84-4b53-a093-4c946e71a4e3",
   "metadata": {},
   "source": [
    "\n",
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312256c-b09b-4fae-8712-37ab2994ced1",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_poscash = POS_CASH_balance_afteng.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10844f66-c49e-4f0c-ae9e-a5c2d46b8bc6",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "nb_nan_median = POS_CASH_balance_afteng[cols_num_poscash].isna().sum().sum()\n",
    "print(f'Number of nan in POS_CASH_balance_afteng before median assignment : {nb_nan_median}')\n",
    "POS_CASH_balance_afteng.fillna(POS_CASH_balance_afteng[cols_num_poscash].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = POS_CASH_balance_afteng[cols_num_poscash].isna().sum().sum()\n",
    "print(f'Number of nan in POS_CASH_balance_afteng after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cebc7-18df-49e7-bd7c-0bd5b47bff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving POS_CASH_balance_afteng\n",
    "with open(path_sav_POS_CASH_balance_afteng, 'wb') as f:\n",
    "    pickle.dump(POS_CASH_balance_afteng, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1d3e0-91a5-4ffb-9df3-1507068e9d60",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='POS_CASH_balance_strong_correl'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>POS_CASH_balance remove strong correlations</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02306b9-c21e-4a7a-ba29-10c632a5e376",
   "metadata": {},
   "source": [
    "In this section:\n",
    " * The correlation between the numerical variables, Pearson's coefficient, is calculated.\n",
    " * The maximum accepted correlation threshold is set at 0.8.\n",
    " * The variables whose correlation exceeds the maximum threshold are drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d4e8d-e867-49d2-bd90-ae26368f0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "with open(path_sav_POS_CASH_balance_afteng, 'rb') as f:\n",
    "    POS_CASH_balance_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1777b-7789-4dad-92e6-9cc41ffbf499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"POS_CASH_balance_afteng\")\n",
    "POS_CASH_balance_afteng =\\\n",
    "    tools_preprocessing.reduce_mem_usage(POS_CASH_balance_afteng, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8305ce-8226-4f3a-86c2-1024aac7063a",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX bureau_merged\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix to avoid having to manage\n",
    "# positive and negative correlations separately\n",
    "corr = POS_CASH_balance_afteng.corr().abs()\n",
    "\n",
    "# Only the part above the diagonal is retained so that\n",
    "# the correlations are taken into account only once (axial symmetry).\n",
    "corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                           .astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2fa8c-d54f-4084-8d82-4f11959098aa",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables with a Pearson coef > 0.8?\n",
    "cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                    if any(corr_triangle[var] > threshold)]\n",
    "print(f'There are {len(cols_corr_a_supp)} variables in POS_CASH_balance_afteng with strong correlation to be removed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af7962-8b47-4c3e-aeca-f18e6aef9433",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'POS_CASH_balance_afteng : {POS_CASH_balance_afteng.shape}')\n",
    "POS_CASH_balance_afteng.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'POS_CASH_balance_afteng : {POS_CASH_balance_afteng.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d3a27-2a62-4bb0-92db-1cd8a3936fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving POS_CASH_balance_afteng ready for ML\n",
    "path_sav_POS_CASH_balance_ML = \\\n",
    "    '../P7_scoring_credit/preprocessing/POS_CASH_balance_ML.pkl'\n",
    "with open(path_sav_POS_CASH_balance_ML, 'wb') as f:\n",
    "    pickle.dump(POS_CASH_balance_afteng, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab4acb-e91e-44b4-8c7f-6610ec0e25a0",
   "metadata": {},
   "source": [
    "**Loading POS_CASH_balance data after pre-processing and feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925f479-e17a-496a-a831-5de093c87433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading bureau_merged\n",
    "with open(path_sav_POS_CASH_balance_ML, 'rb') as df_POS_CASH_balance_ML:\n",
    "    POS_CASH_balance_ML = pickle.load(df_POS_CASH_balance_ML)\n",
    "POS_CASH_balance_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bec7a6-70b1-4917-af67-a52c17991f8c",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748df9c-1a4d-4e77-8d5b-842fe93c991f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['POS_CASH_aftproc.pkl', 'POS_CASH_balance_afteng.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2c6ad-220a-4c3e-83e6-c4c46f0be03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del POS_CASH_balance, POS_CASH_balance_afteng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc10034-4a89-43fc-8b06-10d395e2dc6d",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<a id='installments_payments_preprocessing'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>installments_payments</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962c383-ada5-4f92-9f9d-3e20eb79ccb4",
   "metadata": {},
   "source": [
    "This table contains the details about each installment of client's previous credits with Home Credit Group.\n",
    "<ol><li>We start by sorting the data first by SK_ID_CURR and SK_ID_PREV, and then by NUM_INSTALMENT_NUMBER. This brings the latest installments in the end.</li>\n",
    "    <li>We create some features, such as the number of days the payment was delayed, the difference in amount of payment required vs paid, etc.</li>\n",
    "    <li>Next we aggregate these rows over SK_ID_PREV, such that each client's previous loan gets one row. These aggregations are done in three ways, first overall aggregations, second we aggregate only those installments which were in the last 365 days, and lastly, we aggregate the first 5 installments of every loan. This will help us to capture the starting behaviour, the latest behaviour and the overall behaviour of the client's installments payments.</li>\n",
    "    <li>Now to merge this table with main table, we aggregate the data over SK_ID_CURR.</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aceff18-5b1f-4b6a-b248-a425be910983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Files upload:\n",
    "# -----------------------------\n",
    "# Data from previous loans at Home Credit\n",
    "installments_payments = pd.read_csv('installments_payments.csv',\n",
    "                                    low_memory=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5176b67-b93b-430d-9423-b43f32a411e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"installments_payments\")\n",
    "installments_payments =\\\n",
    "    tools_preprocessing.reduce_mem_usage(installments_payments, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972e66b-ab22-4f94-801a-77f9c1d77154",
   "metadata": {},
   "source": [
    "No outliers were found for this file during the exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b85c65-114a-4793-8090-b7c8b00eced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau_balance after preprocessing\n",
    "path_sav_instpay_aftproc = \\\n",
    "    '../P7_scoring_credit/preprocessing/installments_payments_aftproc.pkl'\n",
    "with open(path_sav_instpay_aftproc, 'wb') as f:\n",
    "    pickle.dump(installments_payments, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657f5b3-547e-410b-a93c-9232acc58f58",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='installments_payments_feat_eng'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>installments_payments feature engineering</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b84b0-f7bc-43d6-8c64-dcd122fc8b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "installments_payments = tools_feat_engineering.feat_eng_installments_payments(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f2103-938f-4ce2-8bf9-80c938d8b02f",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='installments_payments_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>installments_payments assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4f9c6-b976-45ea-8592-1f5e69d17e6c",
   "metadata": {},
   "source": [
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd215ea0-911d-4389-ba5c-908f1a06e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_installments_payments_afteng =  '../P7_scoring_credit/preprocessing/installments_payments_afteng.pkl'\n",
    "\n",
    "with open(path_sav_installments_payments_afteng, 'rb') as f:\n",
    "    installments_payments_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6f417-a220-48c8-8a68-e2a4a0a05ee2",
   "metadata": {},
   "source": [
    "\n",
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b91639-58e5-464f-adc6-b4a14373e4d0",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_poscash = installments_payments_afteng.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322dcfd-09d8-4eb1-bd10-4d9f8d1bf437",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "nb_nan_median = installments_payments_afteng[cols_num_poscash].isna().sum().sum()\n",
    "print(f'Number of nan in installments_payments before median assignment : {nb_nan_median}')\n",
    "installments_payments_afteng.fillna(installments_payments_afteng[cols_num_poscash].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = installments_payments_afteng[cols_num_poscash].isna().sum().sum()\n",
    "print(f'Number of nan in installments_payments_afteng after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc35c7d-3b70-493a-917b-899c110de48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving installments_payments_afteng\n",
    "with open(path_sav_installments_payments_afteng, 'wb') as f:\n",
    "    pickle.dump(installments_payments_afteng, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3a3b7-226b-47a4-9d4d-71bcbf9f9d9b",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='installments_payments_strong_correl'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>installments_payments remove strong correlations</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2252c-e4d5-4545-bea8-38da9dfb3f0d",
   "metadata": {},
   "source": [
    "In this section:\n",
    " * The correlation between the numerical variables, Pearson's coefficient, is calculated.\n",
    " * The maximum accepted correlation threshold is set at 0.8.\n",
    " * The variables whose correlation exceeds the maximum threshold are drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0a1aa-5454-4b02-adb3-b5d3d29e60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "with open(path_sav_installments_payments_afteng, 'rb') as f:\n",
    "    installments_payments_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26a74a-00c3-4661-bdc0-5ba1d31acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"installments_payments_afteng\")\n",
    "installments_payments_afteng =\\\n",
    "    tools_preprocessing.reduce_mem_usage(installments_payments_afteng, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e05f4-d1f2-44a7-9713-990e5cf16778",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX bureau_merged\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix to avoid having to manage\n",
    "# positive and negative correlations separately\n",
    "corr = installments_payments_afteng.corr().abs()\n",
    "\n",
    "# Only the part above the diagonal is retained so that\n",
    "# the correlations are taken into account only once (axial symmetry).\n",
    "corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                           .astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b8bd8-d21b-44f1-835f-0cb5b7a1e739",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables with a Pearson coef > 0.8?\n",
    "cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                    if any(corr_triangle[var] > threshold)]\n",
    "print(f'There are {len(cols_corr_a_supp)} variables in installments_payments_afteng with strong correlation to be removed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a23aa-0868-4ad3-b4a8-996dc85f3f35",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'installments_payments_afteng : {installments_payments_afteng.shape}')\n",
    "installments_payments_afteng.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'installments_payments_afteng : {installments_payments_afteng.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9689771-f18d-4e03-81e2-2b91f8d6de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving installments_payments_afteng ready for ML\n",
    "path_sav_installments_payments_ML = \\\n",
    "    '../P7_scoring_credit/preprocessing/installments_payments_ML.pkl'\n",
    "with open(path_sav_installments_payments_ML, 'wb') as f:\n",
    "    pickle.dump(installments_payments_afteng, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbd2ff-e0b2-4169-8949-ea8aa5b22eb5",
   "metadata": {},
   "source": [
    "**Loading installments_payments data after pre-processing and feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55d0b0-af2d-4d90-ab34-bc352943d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading bureau_merged\n",
    "with open(path_sav_installments_payments_ML, 'rb') as df_installments_payments_ML:\n",
    "    installments_payments_ML = pickle.load(df_installments_payments_ML)\n",
    "installments_payments_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e273d2f-0182-4268-baf5-ea8ba057d358",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d10167-98d8-4415-88a8-9d5b5f0a1039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['installments_payments_aftproc.pkl', 'installments_payments_afteng.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb768570-5ad3-4c41-bb8b-4c26ea19c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del installments_payments, installments_payments_afteng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1107b0-70c5-4efb-93e4-ce8022937aaa",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<a id='cc_balance_preprocessing'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>credit_card_balance</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07053bab-e6a5-426e-8002-b3ddc76ecd61",
   "metadata": {},
   "source": [
    "This table contains information about the previous credit cards that the client had with Home Credit Group.\n",
    "\n",
    "<ol><li>We start off with removing an erroneous value, and then we proceed to feature engineering.</li>\n",
    "    <li>We create some domain based features such as total drawings, number of drawings, balance to limit ratio, payment done to minimum payment required difference, etc.</li>\n",
    "    <li>This table also contains all these data monthwise, so we calculate the EDAs for some of the features of this table too.</li>\n",
    "    <li>For aggregations, we first aggregate over SK_ID_PREV. Here we aggregate on three bases. Firstly, we do overall aggregations. We also do aggregations for last 2 years separately and the rest of the years. Finally we aggregate over SK_ID_PREV for categorical variable NAME_CONTRACT_TYPE. </li>\n",
    "    <li>For aggregation over SK_ID_CURR, we saw from the EDA that most of the current clients just had 1 credit card previously, so we do simple mean aggregations over SK_ID_CURR.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886981c-20d4-4f14-a0ef-65d178e2a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Files upload:\n",
    "# -----------------------------\n",
    "# Data from previous loans at Home Credit\n",
    "cc_balance = pd.read_csv('credit_card_balance.csv',\n",
    "                         low_memory=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eaa0a7-8719-4d25-9d82-4b7ac496fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"cc_balance\")\n",
    "cc_balance =\\\n",
    "    tools_preprocessing.reduce_mem_usage(cc_balance, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d081140-18b3-4023-b36a-9ba0e0096e06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HRTp5goApIcW",
    "outputId": "87a80077-c5b1-43e0-9fa3-7dd2eb67dc3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Variable types\n",
    "# --------------------\n",
    "info_cc_balance =\\\n",
    "    tools_dataframe.complet_description(cc_balance)\n",
    "tools_dataframe.visu_dataTypes(info_cc_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d7a6a-06f2-4286-867b-5f01038898a2",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='cc_balance_preproc'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>credit_card_balance preprocessing</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a026bf-d257-4b32-b1ea-f2378ee692fd",
   "metadata": {},
   "source": [
    "**DAYS_FIRST_DRAWING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca30ea3-8f9d-4710-830b-ed4725ad689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is one abruptly large value for AMT_PAYMENT_CURRENT\n",
    "cc_balance['AMT_PAYMENT_CURRENT'][cc_balance['AMT_PAYMENT_CURRENT'] > 4000000] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e1668-c9ec-4b5c-b00f-48019abaf165",
   "metadata": {},
   "source": [
    "**MISSING_VALS_TOTAL_CC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12928a-f858-4e92-ac94-ce658301a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_balance['MISSING_VALS_TOTAL_CC'] = cc_balance.isna().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3b1f0-bcef-478b-9658-c54e01442ca5",
   "metadata": {},
   "source": [
    "**DAYS_FIRST_DRAWING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be63144-fe09-4109-a9d5-37fbe85b24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the MONTHS_BALANCE Positive\n",
    "cc_balance['MONTHS_BALANCE'] = np.abs(cc_balance['MONTHS_BALANCE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aabf96-c5e2-4e00-8f44-ad6a4c2e40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"cc_balance\")\n",
    "cc_balance =\\\n",
    "    tools_preprocessing.reduce_mem_usage(cc_balance, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce0263-e774-45fd-ba58-bfec86e016bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cc_balance after preprocessing\n",
    "path_sav_cc_balance_aftproc = \\\n",
    "    '../P7_scoring_credit/preprocessing/cc_balance_aftproc.pkl'\n",
    "with open(path_sav_cc_balance_aftproc, 'wb') as f:\n",
    "    pickle.dump(cc_balance, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463dd075-e97e-4aac-ad8f-9c4e43b99f4b",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='cc_balance_feat_eng'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>credit_card_balance feature engineering</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730a30f-6b4e-44b6-9fc4-fa6435bd0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_balance = tools_feat_engineering.feat_eng_cc_balance(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62617b02-70bf-4ac9-a804-b40e156d045d",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='cc_balance_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>credit_card_balance assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1709b5-c6d0-4147-bece-49316bc93886",
   "metadata": {},
   "source": [
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623d415-6e70-441f-befb-3ce9903f4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_cc_balance_afteng =  '../P7_scoring_credit/preprocessing/cc_balance_afteng.pkl'\n",
    "\n",
    "with open(path_sav_cc_balance_afteng, 'rb') as f:\n",
    "    cc_balance_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16a682-bec8-4c13-8965-3b50bf3014b1",
   "metadata": {},
   "source": [
    "\n",
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6225c-8cd1-452d-bc3d-dcc5fde7cb38",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_cc_bal = cc_balance_afteng.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17527da9-d35f-4106-8384-2c8ec9e33545",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "nb_nan_median = cc_balance_afteng[cols_num_cc_bal].isna().sum().sum()\n",
    "print(f'Number of nan in cc_balance_afteng before median assignment : {nb_nan_median}')\n",
    "cc_balance_afteng.fillna(cc_balance_afteng[cols_num_cc_bal].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = cc_balance_afteng[cols_num_cc_bal].isna().sum().sum()\n",
    "print(f'Number of nan in cc_balance_afteng after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55a639-574c-4c1e-b4a6-e10fbdb56556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cc_balance_afteng\n",
    "with open(path_sav_cc_balance_afteng, 'wb') as f:\n",
    "    pickle.dump(cc_balance_afteng, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f9ab04-0f59-41c9-8ae7-dd18cae839fd",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='cc_balance_strong_correl'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>credit_card_balance remove strong correlations</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3ba88-c791-4fb6-a108-863e696fcbe3",
   "metadata": {},
   "source": [
    "In this section:\n",
    " * The correlation between the numerical variables, Pearson's coefficient, is calculated.\n",
    " * The maximum accepted correlation threshold is set at 0.8.\n",
    " * The variables whose correlation exceeds the maximum threshold are drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32dc4be-bcd5-4d8e-96d8-dfd097ba000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "with open(path_sav_cc_balance_afteng, 'rb') as f:\n",
    "    cc_balance_afteng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667179a-97c6-442f-a893-17dc4381d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"cc_balance_afteng\")\n",
    "cc_balance_afteng =\\\n",
    "    tools_preprocessing.reduce_mem_usage(cc_balance_afteng, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c886f-5908-438f-b3e0-d1540a7c793a",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX bureau_merged\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix to avoid having to manage\n",
    "# positive and negative correlations separately\n",
    "corr = cc_balance_afteng.corr().abs()\n",
    "\n",
    "# Only the part above the diagonal is retained so that\n",
    "# the correlations are taken into account only once (axial symmetry).\n",
    "corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                           .astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8a0de-1e9f-458b-8624-dc90aa0a6764",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables with a Pearson coef > 0.8?\n",
    "cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                    if any(corr_triangle[var] > threshold)]\n",
    "print(f'There are {len(cols_corr_a_supp)} variables in cc_balance_afteng with strong correlation to be removed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d32bd2-4f42-427c-b7eb-28bb11944780",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'cc_balance_afteng : {cc_balance_afteng.shape}')\n",
    "cc_balance_afteng.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'cc_balance_afteng : {cc_balance_afteng.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6f0e1-f870-4a64-9e69-ecce972123dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cc_balance_afteng ready for ML\n",
    "path_sav_cc_balance_ML = \\\n",
    "    '../P7_scoring_credit/preprocessing/cc_balance_ML.pkl'\n",
    "with open(path_sav_cc_balance_ML, 'wb') as f:\n",
    "    pickle.dump(cc_balance_afteng, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea953977-1444-4764-ad6c-e14730a1c601",
   "metadata": {},
   "source": [
    "**Loading cc_balance_afteng data after pre-processing and feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb47e58-19b6-47f2-ad26-951e9587e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading bureau_merged\n",
    "with open(path_sav_cc_balance_ML, 'rb') as df_cc_balance_ML:\n",
    "    cc_balance_ML = pickle.load(df_cc_balance_ML)\n",
    "cc_balance_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422bafd-9082-4ed3-8914-98a0b8ac5db0",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1295c0b-ece8-4012-b4bc-b5bf146315f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['cc_balance_aftproc.pkl', 'cc_balance_afteng.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e77172-dc05-4076-aebb-082d29e597e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cc_balance, cc_balance_afteng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac2289-8e4b-434a-920c-a01f9f230eb7",
   "metadata": {
    "id": "2f3cf119-1479-4a49-b958-bb22d513aa1f",
    "tags": []
   },
   "source": [
    "<a id='previous_loans_other_than_Home_Credit'></a>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# <span style='background:#2994ff'><span style='color:white'>**Data from previous loans in financial organisations other than Home Credit Group** </span></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a7ace-f654-43ad-b86b-2c9d2c7dafa4",
   "metadata": {},
   "source": [
    "\n",
    "The 2 files concerned by previous loans in financial organisations other than Home Credit Group are :\n",
    "* **bureau_balance.csv**\n",
    "* **bureau.csv**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f4404-35d7-45c0-999a-54a2a6c9f3a9",
   "metadata": {},
   "source": [
    "These tables contain the information related to the client's previous credits which were not with Home Credit Group, and were reported by Credit Bureau Department. \n",
    "<ol><li><b>bureau_balance</b>\n",
    "    <ol><li>First off, the bureau_balance table contains three fields, i.e. SK_ID_BUREAU, MONTHS_BALANCE and STATUS.</li>\n",
    "        <li>Since the Status follows somewhat ordinal behaviour, we start by label encoding it.</li>\n",
    "        <li>Next, some features are created such as weighted status, which is obtained by dividing the status by the MONTHS_BALANCE.</li>\n",
    "        <li>Since the data contains the timeseries, we also calculate the Exponential Weighted Moving Average of the Status and Weighted Status fields.</li>\n",
    "        <li>Finally, we aggregate the data over SK_ID_BUREAU, in such a way that we first aggregate it over all the data, and after that we also aggregate over the last 2 years. These 2 years would depict the more recent behaviour of the clients.</li>\n",
    "        <li>The aggregations performed are based on Domain Knowledge, such as mean, min, max, sum, count, etc. For EDA features, we only take the last/most recent values, as they somewhat contain the trend of all the previous values.</li></ol>\n",
    "    <li><b>bureau</b>\n",
    "    <ol><li>Firstly, we merge the bureau table with the aggregated bureau_balance table from previous step, on SK_ID_BUREAU.</li>\n",
    "        <li>We replace some erroneous values with NaN values. We saw some loans dating back to as long as 100 years ago. We believe they wouldn't really tell much about client's recent behaviour, so we remove them and only keep the loans in the period of 50 years.</li>\n",
    "        <li>We create some features by multiplications, divisions, subtractions of raw features, based on domain knowledge, such as Credit duration, annutiy to credit ratio, etc.</li>\n",
    "        <li>The categorical features are one-hot encoded.\n",
    "        <li>To merge these to main table, i.e. application_train, we aggregate this table over SK_ID_CURR. We perform the aggregations again in two ways. We aggregate the credits based on the CREDIT_ACTIVE category, where we aggregate for two most popular categories separately, i.e. Active, and Closed. Later we aggregate for the remaining categories too, and merge these. We aggregated the whole data overall too. The aggregations performed are sum, mean, min, max, last, etc.</li>\n",
    "        </ol></li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4670a8-053c-403e-8817-390aafe41d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Files upload:\n",
    "# -----------------------------\n",
    "\n",
    "# Data from previous loans in financial organisations other than Home Credit\n",
    "bureau_balance = pd.read_csv('bureau_balance.csv', low_memory=False,\n",
    "                             encoding='utf-8')\n",
    "bureau = pd.read_csv('bureau.csv', low_memory=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c10a64-6935-42b3-9c16-59184ef2f89d",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<a id='bureau_balance_preprocessing'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>bureau_balance</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fc664-9aab-4bab-a1f0-e153731e1fe6",
   "metadata": {},
   "source": [
    "This file contains information about monthly balances of previous loans in the Credit Bureau.\n",
    "This table has one row for each month of history of each previous credit reported to the Credit Bureau - i.e. the table has (#loans in sample * # of relative previous credits * # of months where we have an observable history for previous credits) rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349b1fb-a6da-46a0-b382-1490231a98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"bureau_balance\")\n",
    "bureau_balance =\\\n",
    "    tools_preprocessing.reduce_mem_usage(bureau_balance, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad87921-fa32-4658-a218-4c31b5b738c6",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='bureau_balance_preproc'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>bureau_balance preprocessing</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83bf21-4e88-47b2-94cb-091aa424e30f",
   "metadata": {},
   "source": [
    "Statut du prêt du Credit Bureau durant le mois (actif ou fermé ou DPD0-30...) :\n",
    "- C => fermé\n",
    "- X => statut inconnu\n",
    "- 0 => pas de DPD\n",
    "- 1 => DPD maximal durant le mois entre 1-30\n",
    "- 2 signifie DPD 31-60\n",
    "- ../.\n",
    "- 5 ==> DPD 120+ ou vendu ou amorti ]\n",
    "\n",
    "L'encodage consistera à donner à la valeur 0 puis d'incrémenter les valeurs, en plaçant X, la valeur inconnue au milieu du tableau des valeurs en lui donnant le bénéfice du doute et 0 pour 0 pas de défaillance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332a848-bc46-4bf8-b889-b69cbd5eef47",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#as we saw from EDA, bureau_balance has a variable called STATUS, which describes about the status of loan.\n",
    "        #it has 7 labels, we will label encode them\n",
    "        #so we give C as 0, and rest increasing\n",
    "        #also we will give X the benefit of doubt and keep it as middle value\n",
    "dict_for_status = { 'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6,\n",
    "                   '5': 7}\n",
    "bureau_balance['STATUS'] = bureau_balance['STATUS'].map(dict_for_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9a4cc-aad5-490d-9711-d9a7e4af39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting months to positive\n",
    "bureau_balance['MONTHS_BALANCE'] = np.abs(bureau_balance['MONTHS_BALANCE'])\n",
    "#weighing the status with the months_balance\n",
    "bureau_balance['WEIGHTED_STATUS'] = bureau_balance.STATUS / (bureau_balance.MONTHS_BALANCE + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa0f96-2d79-4ad3-a275-16180dc1aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"bureau_balance\")\n",
    "bureau_balance =\\\n",
    "    tools_preprocessing.reduce_mem_usage(bureau_balance, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105bb860-06a8-4510-a7e9-a94ed7262e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau_balance after preprocessing\n",
    "path_sav_bureaubal_aftproc = \\\n",
    "    '../P7_scoring_credit/preprocessing/bureaubal_aftproc.pkl'\n",
    "with open(path_sav_bureaubal_aftproc, 'wb') as f:\n",
    "    pickle.dump(bureau_balance, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c584e03-b497-4bea-b565-bc18c9da0245",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='bureau_balance_feat_eng'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>bureau_balance feature engineering</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022f2f8-23e3-4609-a68b-ef426c7159da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = tools_feat_engineering.feat_eng_bureau_balance(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef752d42-79ec-4fb7-af51-8a1ad5d51e63",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='bureau_balance_assing_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>bureau_balance assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f247ea-33f6-453f-8e92-15c35cf31868",
   "metadata": {},
   "source": [
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321d52f-314a-4f6e-b144-03a70059181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_bureaubal_aft_feateng =  '../P7_scoring_credit/preprocessing/bureau_balance_aft_feat_eng.pkl'\n",
    "\n",
    "with open(path_sav_bureaubal_aft_feateng, 'rb') as f:\n",
    "    aggr_bureau_balance = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09acc3-77fb-4f51-8896-825cee5352fb",
   "metadata": {},
   "source": [
    "\n",
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bad114-0be6-4a74-9b4e-af6954a759ec",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_bureaub = aggr_bureau_balance.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cfa9f-48c1-488d-a45a-6135ae039328",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "nb_nan_median = aggr_bureau_balance[cols_num_bureaub].isna().sum().sum()\n",
    "print(f'Number of nan in aggr_bureau_balance before median assignment : {nb_nan_median}')\n",
    "aggr_bureau_balance.fillna(aggr_bureau_balance[cols_num_bureaub].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = aggr_bureau_balance[cols_num_bureaub].isna().sum().sum()\n",
    "print(f'Number of nan in aggr_bureau_balance after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d254322f-2d3f-4385-9f0d-71367b19a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving aggr_bureau_balance\n",
    "with open(path_sav_bureaubal_aft_feateng, 'wb') as f:\n",
    "    pickle.dump(aggr_bureau_balance, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5850cf61-902c-481c-baf6-8f7ef37d017d",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36553b51-2ec6-4942-ab6f-401b51048ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['bureaubal_aftproc.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c613cca-c21c-46d5-af64-bea7cad7a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del bureau_balance, aggr_bureau_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a931e-9f50-4ba7-94c6-851970e7475e",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<a id='bureau_preprocessing_feat_eng'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>bureau</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d1eeb-a263-4d7b-a63f-70480e52b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"bureau\")\n",
    "bureau =\\\n",
    "    tools_preprocessing.reduce_mem_usage(bureau, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c22467-4dec-41b4-bc98-e7545fbad24f",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='bureau'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>bureau preprocessing</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac1e8f-be5c-4309-b090-6904ea06c29f",
   "metadata": {},
   "source": [
    "<a id='bureau_outliers'></a>\n",
    "\n",
    "---\n",
    "#### <span style='background:#0075bc'><span style='color:white'>bureau outliers</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b627b-0af6-447b-8d18-c7ff06f4d13f",
   "metadata": {},
   "source": [
    "From the EDA we saw some erroneous values in DAYS Fields, we will remove those there are some loans which ended about very long ago, around 100 years ago.\n",
    "Only those loans which have ended in past 50 years will be retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47439542-9c4a-44c3-a6cd-7a827f10d56b",
   "metadata": {},
   "source": [
    "**DAYS_CREDIT_ENDDATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf681cd-7b2b-40c3-a700-c50dfc35d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove information related to loans that ended more than 50 years ago\n",
    "bureau['DAYS_CREDIT_ENDDATE'][bureau['DAYS_CREDIT_ENDDATE'] > -50*365] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61caea2-e781-424d-9198-7d2ac1452838",
   "metadata": {},
   "source": [
    "**DAYS_ENDDATE_FACT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9591a-cd5a-43e2-beda-df65e9a83fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove information related to loans that ended more than 50 years ago\n",
    "bureau['DAYS_ENDDATE_FACT'][bureau['DAYS_ENDDATE_FACT'] > -50*365] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411aa503-29c7-40ca-b409-2676f03df880",
   "metadata": {},
   "source": [
    "**DAYS_CREDIT_UPDATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d238f-f962-4df4-baa0-3bff5757a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove information related to loans that ended more than 50 years ago\n",
    "bureau['DAYS_CREDIT_UPDATE'][bureau['DAYS_CREDIT_UPDATE'] > -50*365] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b3dee-1d2c-4b95-b1c8-6dae9d9e2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"bureau\")\n",
    "bureau = tools_preprocessing.reduce_mem_usage(bureau, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3a5ec-8408-4793-aa3f-38b7b4ec9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau after preprocessing\n",
    "path_sav_bureau_aftproc = \\\n",
    "    '../P7_scoring_credit/preprocessing/bureau_aftproc.pkl'\n",
    "with open(path_sav_bureau_aftproc, 'wb') as f:\n",
    "    pickle.dump(bureau, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2d831-88ca-4ecf-80d2-db7dbeedead8",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='bureau_feat_eng'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>bureau feature engineering</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c550a-c5df-42aa-a09d-eaaec1b5b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = tools_feat_engineering.feat_eng_bureau(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf35fe-7dff-4ee6-b888-e1df613775d0",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='bureau_merged_assing_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>bureau_merged assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5b8a8-9732-4c0c-893c-e679b638d968",
   "metadata": {},
   "source": [
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885da12-7d79-4cc5-b6b4-cb4210dfdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de dataset after feature engineering\n",
    "path_sav_bureau_merged_aggregated =  '../P7_scoring_credit/preprocessing/bureau_merged_aggregated.pkl'\n",
    "\n",
    "with open(path_sav_bureau_merged_aggregated, 'rb') as f:\n",
    "    bureau_merged_aggregated = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427299b3-bee6-42b3-a6d4-4b13b5451217",
   "metadata": {},
   "source": [
    "\n",
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9e034-89f1-4308-9c35-4cdfd0881d38",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_bureaumerg = bureau_merged_aggregated.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad8365-d975-456b-9864-12c6336176ff",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "nb_nan_median = bureau_merged_aggregated[cols_num_bureaumerg].isna().sum().sum()\n",
    "print(f'Number of nan in bureau_merged_aggregated before median assignment : {nb_nan_median}')\n",
    "bureau_merged_aggregated.fillna(bureau_merged_aggregated[cols_num_bureaumerg].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = bureau_merged_aggregated[cols_num_bureaumerg].isna().sum().sum()\n",
    "print(f'Number of nan in bureau_merged_aggregated after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f32e8-4eac-43ab-8407-64f64c38080c",
   "metadata": {},
   "source": [
    "**Categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41441e6f-b339-494f-bd14-afac4e31c022",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cols_cat_bureaumerg = bureau_merged_aggregated.select_dtypes(exclude=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "cols_cat_bureaumerg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136460d-482c-4c37-90b1-8265ece1cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau_merged_aggregated\n",
    "with open(path_sav_bureaubal_aft_feateng, 'wb') as f:\n",
    "    pickle.dump(bureau_merged_aggregated, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9bfe6-526d-455d-85a6-0533b3057ab4",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32be829-8792-4275-a75d-6f570617f59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['bureau_balance_aft_feat_eng.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e88d13-dc07-4bba-8e8d-2e6e3b2bc3ba",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "toc-hr-collapsed": true
   },
   "source": [
    "<a id='bureau_balance_bureau_strong_correl'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>bureau merged strong correlations</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf47519-38f4-47e2-a65c-bc46d535cc86",
   "metadata": {},
   "source": [
    "In this section:\n",
    " * The correlation between the numerical variables, Pearson's coefficient, is calculated.\n",
    " * The maximum accepted correlation threshold is set at 0.8.\n",
    " * The variables whose correlation exceeds the maximum threshold are drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c261e-41ed-457f-be7b-2aa010145549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Memory improvement by reducing the size of objects\n",
    "# ----------------------------------------------------\n",
    "print(\"bureau_merged_aggregated\")\n",
    "bureau_merged_aggregated =\\\n",
    "    tools_preprocessing.reduce_mem_usage(bureau_merged_aggregated, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ba7fc-2f40-44b5-b70e-8b813afccf6f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX bureau_merged\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix to avoid having to manage\n",
    "# positive and negative correlations separately\n",
    "corr = bureau_merged_aggregated.corr().abs()\n",
    "\n",
    "# Only the part above the diagonal is retained so that\n",
    "# the correlations are taken into account only once (axial symmetry).\n",
    "corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                           .astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbc84f-46d3-417a-83a2-526dd438e978",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables with a Pearson coef > 0.8?\n",
    "cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                    if any(corr_triangle[var] > threshold)]\n",
    "print(f'There are {len(cols_corr_a_supp)} variables in bureau_merged_aggregated with strong correlation to be removed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4fe8b-eae0-4440-8b8e-3a047a842f48",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'bureau_merged_aggregated : {bureau_merged_aggregated.shape}')\n",
    "bureau_merged_aggregated.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'bureau_merged_aggregated : {bureau_merged_aggregated.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c1979-3791-44d4-8b47-edc1bbdb4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau_merged_aggregated ready for ML\n",
    "path_sav_bureau_ML = \\\n",
    "    '../P7_scoring_credit/preprocessing/bureau_ML.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60299e-377c-496f-ae85-61ab96d41659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving bureau_merged_aggregated ready for ML\n",
    "\n",
    "with open(path_sav_bureau_ML, 'wb') as f:\n",
    "    pickle.dump(bureau_merged_aggregated, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779d1ef-76cf-49b1-a99d-1cea438b6846",
   "metadata": {},
   "source": [
    "**Loading bureau data after pre-processing and feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3aed8-bb87-4022-9461-4d996eb22b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading bureau_merged\n",
    "with open(path_sav_bureau_ML, 'rb') as df_bureau_merged_ML:\n",
    "    bureau_ML = pickle.load(df_bureau_merged_ML)\n",
    "bureau_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b5a29-d158-4cea-978f-90f1356ac3a2",
   "metadata": {},
   "source": [
    "**Freeing up the memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9169e6-3d9f-459e-be4a-b72a2500c5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['bureau_aftproc.pkl', 'bureau_merged_aggregated.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f69765-edc2-45c7-8670-13fc7955419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del bureau, bureau_merged_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884de1ea-c01e-437a-af3d-5278b19a5280",
   "metadata": {
    "id": "2f3cf119-1479-4a49-b958-bb22d513aa1f",
    "tags": []
   },
   "source": [
    "<a id='merging_tables'></a>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# <span style='background:#2994ff'><span style='color:white'>**Merging All Tables** </span></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465149b-c068-4d0c-aba9-813db8261a94",
   "metadata": {},
   "source": [
    "Now we will merge all the preprocessed tables with the application_train and application_test tables. The merges will be Left Outer Joins, such that all the current applications are preserved, as we have to model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd49cee-47fe-42b9-8057-9b89fc89545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de datasets after feature engineering\n",
    "path =  '../P7_scoring_credit/preprocessing/'\n",
    "\n",
    "with open(path + 'application_train_ML.pkl', 'rb') as f:\n",
    "    application_train_ML = pickle.load(f)\n",
    "with open(path + 'application_test_ML.pkl', 'rb') as f:\n",
    "    application_test_ML = pickle.load(f)\n",
    "with open(path + 'bureau_ML.pkl', 'rb') as f:\n",
    "    bureau_ML = pickle.load(f)\n",
    "with open(path + 'previous_application_ML.pkl', 'rb') as f:\n",
    "    previous_application_ML = pickle.load(f)\n",
    "with open(path + 'POS_CASH_balance_ML.pkl', 'rb') as f:\n",
    "    POS_CASH_balance_ML = pickle.load(f)\n",
    "with open(path + 'installments_payments_ML.pkl', 'rb') as f:\n",
    "    installments_payments_ML = pickle.load(f)\n",
    "with open(path + 'cc_balance_ML.pkl', 'rb') as f:\n",
    "    cc_balance_ML = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01ca6c1-aac0-450f-a6d2-c9ae828271e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size of train_data (307511, 195)\n",
      "\n",
      "Initial size of test_data (48744, 194)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial size of train_data {application_train_ML.shape}\")\n",
    "print(f\"\\nInitial size of test_data {application_test_ML.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee0d711-299a-44c3-ab7b-288256884a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T09:06:40.873978Z",
     "start_time": "2020-10-23T09:04:28.790785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Memory usage of dataframe is: 539.02 MB\n",
      "Memory usage after optimization is : 502.07 MB\n",
      "Reduction of 6.9%\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Memory usage of dataframe is: 84.65 MB\n",
      "Memory usage after optimization is : 77.40 MB\n",
      "Reduction of 8.6%\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = \\\n",
    "    tools_preprocessing.merge_all_tables(application_train_ML,\n",
    "                                         application_test_ML,\n",
    "                                         bureau_ML,\n",
    "                                         previous_application_ML,\n",
    "                                         POS_CASH_balance_ML,\n",
    "                                         installments_payments_ML,\n",
    "                                         cc_balance_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d215d-3cf1-42fb-8616-711acec60c11",
   "metadata": {
    "id": "UjzpPUhyDxT8",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<a id='feat_interaction_diff_tables'></a>\n",
    "\n",
    "## <span style='background:#0085dd'><span style='color:white'>Features Based on Interaction Among Different Tables</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150f8e9-355e-418a-90fa-de221724cccc",
   "metadata": {},
   "source": [
    "We will create some more features based on interactions between different tables. For example, we will calculate the Annuity to income ratio for previous applications, similarly we will calculate Credit to income ratios, and several such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663f3474-c260-4fc6-a2a4-31711904144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size of train_data (307511, 610)\n",
      "\n",
      "Initial size of test_data (48744, 609)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial size of train_data {train_data.shape}\")\n",
    "print(f\"\\nInitial size of test_data {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49590d9-270e-48c5-9789-b3f4c8f880d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T16:04:25.854127Z",
     "start_time": "2020-10-24T16:04:25.837198Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_new_features(data):\n",
    "    '''\n",
    "    Function to create few more features after the merging of features, by using the\n",
    "    interactions between various tables.\n",
    "    \n",
    "    Inputs:\n",
    "        data: DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    #previous applications columns\n",
    "    prev_annuity_columns = [ele for ele in previous_application_ML.columns if 'AMT_ANNUITY' in ele]\n",
    "    for col in prev_annuity_columns:\n",
    "        data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    prev_goods_columns = [ele for ele in previous_application_ML.columns if 'AMT_GOODS' in ele]\n",
    "    for col in prev_goods_columns:\n",
    "        data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "  \n",
    "    #credit_card_balance columns\n",
    "    cc_amt_principal_cols = [ele for ele in cc_balance_ML.columns if 'AMT_RECEIVABLE_PRINCIPAL' in ele]\n",
    "    for col in cc_amt_principal_cols:\n",
    "        data['CC_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    cc_amt_recivable_cols = [ele for ele in cc_balance_ML.columns if 'AMT_RECIVABLE' in ele]\n",
    "    for col in cc_amt_recivable_cols:\n",
    "        data['CC_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    cc_amt_total_receivable_cols = [ele for ele in cc_balance_ML.columns if 'TOTAL_RECEIVABLE' in ele]\n",
    "    for col in cc_amt_total_receivable_cols:\n",
    "        data['CC_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    \n",
    "    #installments_payments columns\n",
    "    installments_payment_cols = [ele for ele in installments_payments_ML.columns if 'AMT_PAYMENT' in ele and 'RATIO' not in ele and 'DIFF' not in ele]\n",
    "    for col in installments_payment_cols:\n",
    "        data['INSTALLMENTS_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    \n",
    "    #POS_CASH_balance features have been created in its own dataframe itself\n",
    "\n",
    "    #bureau and bureau_balance columns\n",
    "    bureau_days_credit_cols = [ele for ele in bureau_ML.columns if 'DAYS_CREDIT' in ele and 'ENDDATE' not in ele and 'UPDATE' not in ele]\n",
    "    for col in bureau_days_credit_cols:\n",
    "        data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
    "        data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']  \n",
    "    bureau_overdue_cols = [ele for ele in bureau_ML.columns if 'AMT_CREDIT' in ele and 'OVERDUE' in ele]\n",
    "    for col in bureau_overdue_cols:\n",
    "        data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    bureau_amt_annuity_cols = [ele for ele in bureau_ML.columns if 'AMT_ANNUITY' in ele and 'CREDIT'  not in ele]\n",
    "    for col in bureau_amt_annuity_cols:\n",
    "        data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37fc488-19a1-45dd-932e-5c5b79178223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T09:06:45.235385Z",
     "start_time": "2020-10-23T09:06:40.946783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Pre-processing, aggregation, merging and Feature Engineering,\n",
      "Final Shape of Training Data = (307511, 636)\n",
      "Final Shape of Test Data = (48744, 635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
      "/tmp/ipykernel_5803/1115651349.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
      "/tmp/ipykernel_5803/1115651349.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n"
     ]
    }
   ],
   "source": [
    "create_new_features(train_data)\n",
    "create_new_features(test_data)\n",
    "\n",
    "print(\"After Pre-processing, aggregation, merging and Feature Engineering,\")\n",
    "print(f\"Final Shape of Training Data = {train_data.shape}\")\n",
    "print(f\"Final Shape of Test Data = {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f0a98f-4cc8-4ffa-931a-bc71fecdffdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T09:06:45.235385Z",
     "start_time": "2020-10-23T09:06:40.946783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#freeing up the memory\n",
    "del application_train_ML, application_test_ML, bureau_ML, previous_application_ML, installments_payments_ML, POS_CASH_balance_ML, cc_balance_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f7ad5-2a80-4188-9e0d-46a7983c3f87",
   "metadata": {
    "id": "UjzpPUhyDxT8"
   },
   "source": [
    "<a id='dataset_merged_assing_missing_values'></a>\n",
    "\n",
    "### <span style='background:#0075bc'><span style='color:white'>Merged tables assignment missing values</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f178159-4489-4bc2-a4c9-f1fc1859b865",
   "metadata": {},
   "source": [
    "* The **missing values** of the **continuous variables** will be therefore imputed by the **median** of each of these variables.\n",
    "* The **missing values** of the **categorical variables** will be therefore imputed by the **mode** of each of these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd7efc-c4f2-449e-ac5a-bd7faf560d6f",
   "metadata": {},
   "source": [
    "\n",
    "**Continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d78c0d23-86c8-4708-9edd-d9c7399d2c55",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Continous variables\n",
    "cols_num_train = train_data.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()\n",
    "cols_num_test = test_data.select_dtypes(include=[np.number])\\\n",
    "                                    .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "454d5f2e-9f97-4e71-9ebb-70ea71734811",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan in train_data before median assignment : 37438923\n",
      "Number of nan in train_data after median assignment : 7072753\n"
     ]
    }
   ],
   "source": [
    "nb_nan_median = train_data[cols_num_train].isna().sum().sum()\n",
    "print(f'Number of nan in train_data before median assignment : {nb_nan_median}')\n",
    "train_data.fillna(train_data[cols_num_train].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = train_data[cols_num_train].isna().sum().sum()\n",
    "print(f'Number of nan in train_data after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4803f5f-3a5b-43c6-a826-2c256587ec18",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan in test_data before median assignment : 5301793\n",
      "Number of nan in test_data after median assignment : 1121112\n"
     ]
    }
   ],
   "source": [
    "nb_nan_median = test_data[cols_num_test].isna().sum().sum()\n",
    "print(f'Number of nan in test_data before median assignment : {nb_nan_median}')\n",
    "test_data.fillna(test_data[cols_num_test].median(), inplace=True)\n",
    "# Validation\n",
    "nb_nan_median = test_data[cols_num_test].isna().sum().sum()\n",
    "print(f'Number of nan in test_data after median assignment : {nb_nan_median}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84bb35ef-519a-4995-8960-3d85b088e182",
   "metadata": {},
   "source": [
    "# Load de datasets after feature engineering\n",
    "path =  '../P7_scoring_credit/preprocessing/'\n",
    "\n",
    "with open(path + 'final_train_data.pkl', 'rb') as f:\n",
    "    final_train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "301fee57-3b03-4509-9723-77feb67466e5",
   "metadata": {},
   "source": [
    "\n",
    "with open(path + 'final_test_data.pkl', 'rb') as f:\n",
    "    final_test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48ed993f-7449-4687-adb7-33db1760b1d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T06:12:57.891910Z",
     "start_time": "2020-10-26T06:12:56.211057Z"
    }
   },
   "source": [
    "#extracting the SK_ID_CURR from training and test data\n",
    "skid_train = train_data['SK_ID_CURR']\n",
    "skid_test = test_data['SK_ID_CURR']\n",
    "\n",
    "# Suppression des identifiants\n",
    "train_data = train_data.drop(columns=['SK_ID_CURR'])\n",
    "test_data = test_data.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "#extracting the class labels for training data\n",
    "target_train = train_data['TARGET']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed261b0-a7e5-4143-8fb4-6dfb1afef7f7",
   "metadata": {},
   "source": [
    "#standardizing the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#np.seterr(invalid='ignore')\n",
    "\n",
    "train_data_std = scaler.fit_transform(train_data)\n",
    "test_data_std = scaler.fit_transform(test_data)\n",
    "\n",
    "#replacing nan values with 0\n",
    "# x_train_std[np.isnan(x_train_std)] = 0\n",
    "# x_test_std[np.isnan(x_test_std)] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdde0faa-6ddb-4fa5-8446-01df4a391721",
   "metadata": {},
   "source": [
    "#replacing nan values with 0\n",
    "x_train_std[np.isnan(x_train_std)] = 0\n",
    "x_test_std[np.isnan(x_test_std)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdc404-91ff-494d-9236-8ccb2eb3db79",
   "metadata": {},
   "source": [
    "**Identify Strong Correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff7924a0-55f4-4e4c-b19b-3773be41fd1f",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5803/4067751850.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  .astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "# CORRELATION MATRIX application_train\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "# Absolute value correlation matrix to avoid having to manage\n",
    "# positive and negative correlations separately\n",
    "corr = train_data.corr().abs()\n",
    "\n",
    "# Only the part above the diagonal is retained so that\n",
    "# the correlations are taken into account only once (axial symmetry).\n",
    "corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                           .astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a32dc32-d6c8-42cb-8d94-000990331b6f",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 variables in train_data with strong correlation to be removed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variables with a Pearson coef > 0.8?\n",
    "cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                    if any(corr_triangle[var] > threshold)]\n",
    "print(f'There are {len(cols_corr_a_supp)} variables in train_data with strong correlation to be removed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f994df3-f2ae-475e-a6da-33d7d67ad06c",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data : (307511, 636)\n",
      "train_data : (307511, 603)\n"
     ]
    }
   ],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'train_data : {train_data.shape}')\n",
    "train_data.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'train_data : {train_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b7c5a86-4ffc-4894-b847-ad1a5976ed5e",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data : (48744, 635)\n",
      "test_data : (48744, 602)\n"
     ]
    }
   ],
   "source": [
    "# Drop vairables with strong correlation\n",
    "print(f'test_data : {test_data.shape}')\n",
    "test_data.drop(columns=cols_corr_a_supp,  inplace=True)\n",
    "print(f'test_data : {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc4151-0ca3-4021-99bd-081655872f71",
   "metadata": {},
   "source": [
    "**Look for empty columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a347b034-91b9-4c2c-964d-bf50693d6490",
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1687284795661,
     "user": {
      "displayName": "Raquel Sanchez-Pellicer",
      "userId": "01557635450717251675"
     },
     "user_tz": -120
    },
    "id": "457583a6-6451-48c6-94a9-3d3bb9585fae"
   },
   "outputs": [],
   "source": [
    "# Define the folder containing the files with the project data\n",
    "P7_scoring_credit = \"/home/raquelsp/Documents/Openclassrooms/P7_implementez_modele_scoring/P7_travail/P7_scoring_credit/\"\n",
    "\n",
    "os.chdir(P7_scoring_credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f142dd89-9827-4395-9ef1-da2b14b22984",
   "metadata": {
    "executionInfo": {
     "elapsed": 5935,
     "status": "ok",
     "timestamp": 1687284801564,
     "user": {
      "displayName": "Raquel Sanchez-Pellicer",
      "userId": "01557635450717251675"
     },
     "user_tz": -120
    },
    "id": "cb925c60-1983-450e-a4bd-4c872aad3e09"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Files loading:\n",
    "# -----------------------------\n",
    "\n",
    "# Open final train_dataset : these will be our reference data\n",
    "path_train_data = \\\n",
    "    'preprocessing/final_train_data.pkl'\n",
    "\n",
    "with open(path_train_data, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Open final test_dataset : these will be our current data\n",
    "path_test_data = \\\n",
    "    'preprocessing/final_test_data.pkl'\n",
    "with open(path_test_data, 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28939dbe-e71e-4f1c-8e75-38b4d1a67d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Column description\n",
    "# --------------------\n",
    "info_train_data = tools_dataframe.complet_description(train_data)\n",
    "info_train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f4654-ad86-440b-b9c5-ee4d9829c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify empty columns\n",
    "to_remove = info_train_data.loc[info_train_data['Filling percentage']<1]\n",
    "cols_to_remove = to_remove['Variable'].tolist()\n",
    "print(f'There are {len(cols_to_remove)} empty columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96151480-f359-4736-8d9d-98fe5ebc51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty columns\n",
    "train_data = \\\n",
    "    train_data[train_data.columns[~train_data.columns.isin(cols_to_remove)]]\n",
    "test_data = \\\n",
    "    test_data[test_data.columns[~test_data.columns.isin(cols_to_remove)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1cceb9-693b-4117-9962-c34e86699247",
   "metadata": {},
   "source": [
    "**Save final datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b533f852-203b-445b-95a2-04fa56f1ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving final dataset for train data\n",
    "path_sav_alldata_train_merged = \\\n",
    "    '../P7_scoring_credit/preprocessing/final_train_data.pkl'\n",
    "with open(path_sav_alldata_train_merged, 'wb') as f:\n",
    "    pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bdb3b2a-b74a-431e-b613-342d26e75d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving final dataset for test data\n",
    "path_sav_alldata_test_merged = \\\n",
    "    '../P7_scoring_credit/preprocessing/final_test_data.pkl'\n",
    "with open(path_sav_alldata_test_merged, 'wb') as f:\n",
    "    pickle.dump(test_data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ded499-f67c-451f-9054-bcde34646c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_processing = '../P7_scoring_credit/preprocessing/'\n",
    "files_to_remove = ['application_train_ML.pkl', 'application_test_ML.pkl', 'bureau_ML.pkl', 'previous_application_ML.pkl', 'POS_CASH_balance_ML.pkl', 'installments_payments_ML.pkl', 'cc_balance_ML.pkl']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    os.remove(path_processing+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca60877-48fc-47ad-9ef8-abddaf15f555",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026ce6a-a68f-4c63-b89b-6596a4538093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
